{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8o3Vnxl-LiN"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "Name: Vivek Mule\n",
        "Roll: 381072\n",
        "PRN: 22420145\n",
        "\n",
        "Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on data. Create embeddings using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nBbuLCK_Q9b",
        "outputId": "8f5714dc-55cf-4801-db93-8cfe224be755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (4.3.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk gensim scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2k_S_ep_Ubo",
        "outputId": "9fcae125-e9e9-4a3d-d2d0-2b9bfc02bdf4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#imports\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhTk-QZd_W6T",
        "outputId": "a7de1496-3379-442b-d830-d25aff707cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Using NLTK for text processing', 'NLTK provides easy-to-use interfaces', 'Pyton is great for NLP tasks', 'NLP includes tokenization, stemming, and more']\n"
          ]
        }
      ],
      "source": [
        "# sample dataset\n",
        "\n",
        "documents = [\n",
        "    \"Using NLTK for text processing\",\n",
        "    \"NLTK provides easy-to-use interfaces\",\n",
        "    \"Pyton is great for NLP tasks\",\n",
        "    \"NLP includes tokenization, stemming, and more\"\n",
        "]\n",
        "\n",
        "print(documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_iUVQfY_am0",
        "outputId": "da16ed9b-16f9-433e-ff9b-8f98b7faf1a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            "['and' 'easy' 'for' 'great' 'includes' 'interfaces' 'is' 'more' 'nlp'\n",
            " 'nltk' 'processing' 'provides' 'pyton' 'stemming' 'tasks' 'text' 'to'\n",
            " 'tokenization' 'use' 'using']\n",
            "\n",
            "Bag-of-Words (Count Occurrence):\n",
            "[[0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1]\n",
            " [0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0]\n",
            " [0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0]\n",
            " [1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0]]\n"
          ]
        }
      ],
      "source": [
        "#Bag-of-Words (Count Occurrence)\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_counts = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(count_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag-of-Words (Count Occurrence):\")\n",
        "print(bow_counts.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXlQD38A_gpX",
        "outputId": "e22d5907-4faf-42bb-b11f-d910430200ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            "['ai' 'and' 'are' 'fascinating' 'field' 'is' 'learning' 'machine' 'nlp'\n",
            " 'of' 'part' 'python' 'the' 'transforming' 'using' 'we' 'world']\n",
            "\n",
            "Normalized Bag-of-Words (L2 Normalization):\n",
            "[[0.40824829 0.         0.         0.40824829 0.40824829 0.40824829\n",
            "  0.         0.         0.40824829 0.40824829 0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.37796447 0.37796447 0.37796447 0.         0.         0.\n",
            "  0.         0.         0.37796447 0.         0.         0.\n",
            "  0.37796447 0.37796447 0.         0.         0.37796447]\n",
            " [0.40824829 0.         0.         0.         0.         0.40824829\n",
            "  0.40824829 0.40824829 0.         0.40824829 0.40824829 0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.40824829 0.         0.         0.\n",
            "  0.40824829 0.         0.40824829 0.         0.         0.40824829\n",
            "  0.         0.         0.40824829 0.40824829 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Bag-of-Words (Normalized Count Occurrence) (L2 Normalization)\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "count_vectorizer_norm = CountVectorizer()\n",
        "bow_counts = count_vectorizer_norm.fit_transform(documents)\n",
        "bow_normalized = normalize(bow_counts, norm='l2', axis=1)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(count_vectorizer_norm.get_feature_names_out())\n",
        "\n",
        "print(\"\\nNormalized Bag-of-Words (L2 Normalization):\")\n",
        "print(bow_normalized.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWfxaNRA_nt7",
        "outputId": "19d89f6a-cfeb-4314-9f48-718a4e08881e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            "['and' 'easy' 'for' 'great' 'includes' 'interfaces' 'is' 'more' 'nlp'\n",
            " 'nltk' 'processing' 'provides' 'pyton' 'stemming' 'tasks' 'text' 'to'\n",
            " 'tokenization' 'use' 'using']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.38274272 0.         0.         0.\n",
            "  0.         0.         0.         0.38274272 0.48546061 0.\n",
            "  0.         0.         0.         0.48546061 0.         0.\n",
            "  0.         0.48546061]\n",
            " [0.         0.42176478 0.         0.         0.         0.42176478\n",
            "  0.         0.         0.         0.3325242  0.         0.42176478\n",
            "  0.         0.         0.         0.         0.42176478 0.\n",
            "  0.42176478 0.        ]\n",
            " [0.         0.         0.34431452 0.43671931 0.         0.\n",
            "  0.43671931 0.         0.34431452 0.         0.         0.\n",
            "  0.43671931 0.         0.43671931 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.42176478 0.         0.         0.         0.42176478 0.\n",
            "  0.         0.42176478 0.3325242  0.         0.         0.\n",
            "  0.         0.42176478 0.         0.         0.         0.42176478\n",
            "  0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF Representation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvo8W13R_yvF",
        "outputId": "83540e81-7a2c-4a22-ac27-692a5107474d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['nlp', 'is', 'a', 'fascinating', 'field', 'of', 'ai'], ['ai', 'and', 'nlp', 'are', 'transforming', 'the', 'world'], ['machine', 'learning', 'is', 'part', 'of', 'ai'], ['we', 'are', 'learning', 'nlp', 'using', 'python']]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize Sentences for Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "print(tokenized_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5Wpbd69_1Ro"
      },
      "outputs": [],
      "source": [
        "# Train Word2Vec Model\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_docs,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC1XaeoXABkN",
        "outputId": "4c5c890f-5991-47a7-c417-5f8b16989478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector for word 'nlp':\n",
            "[-8.6207762e-03  3.6693334e-03  5.1942971e-03  5.7473937e-03\n",
            "  7.4650599e-03 -6.1747055e-03  1.1099416e-03  6.0544759e-03\n",
            " -2.8448051e-03 -6.1771143e-03 -4.0740482e-04 -8.3730584e-03\n",
            " -5.6021605e-03  7.1088444e-03  3.3524618e-03  7.2231065e-03\n",
            "  6.8022693e-03  7.5322362e-03 -3.7926226e-03 -5.6901621e-04\n",
            "  2.3529716e-03 -4.5188968e-03  8.3920360e-03 -9.8620411e-03\n",
            "  6.7665880e-03  2.9115782e-03 -4.9358848e-03  4.4020070e-03\n",
            " -1.7429485e-03  6.7098825e-03  9.9619338e-03 -4.3645748e-03\n",
            " -5.9599307e-04 -5.6999368e-03  3.8509031e-03  2.7887921e-03\n",
            "  6.8953354e-03  6.1001154e-03  9.5395697e-03  9.2723612e-03\n",
            "  7.8964084e-03 -6.9908407e-03 -9.1608996e-03 -3.5524677e-04\n",
            " -3.1000818e-03  7.8951921e-03  5.9356242e-03 -1.5428711e-03\n",
            "  1.5138414e-03  1.7952634e-03  7.8164274e-03 -9.5088966e-03\n",
            " -2.0412030e-04  3.4708700e-03 -9.3429489e-04  8.3816377e-03\n",
            "  9.0165604e-03  6.5354626e-03 -7.1288715e-04  7.7175130e-03\n",
            " -8.5373772e-03  3.2064954e-03 -4.6427255e-03 -5.0939051e-03\n",
            "  3.5908883e-03  5.3730384e-03  7.7714743e-03 -5.7624332e-03\n",
            "  7.4343458e-03  6.6272845e-03 -3.7086469e-03 -8.7436540e-03\n",
            "  5.4394249e-03  6.5085166e-03 -7.8311260e-04 -6.7079277e-03\n",
            " -7.0828749e-03 -2.4959052e-03  5.1439563e-03 -3.6696102e-03\n",
            " -9.3743131e-03  3.8316771e-03  4.8879450e-03 -6.4266520e-03\n",
            "  1.2078680e-03 -2.0728749e-03  2.7815922e-05 -9.8887291e-03\n",
            "  2.6948445e-03 -4.7458615e-03  1.0906652e-03 -1.5757994e-03\n",
            "  2.1983718e-03 -7.8840861e-03 -2.7113969e-03  2.6675414e-03\n",
            "  5.3426516e-03 -2.3951977e-03 -9.5107164e-03  4.5088506e-03]\n",
            "\n",
            "Vector size: 100\n"
          ]
        }
      ],
      "source": [
        "# Word Embeddings (Vector Representation)\n",
        "print(\"Vector for word 'nlp':\")\n",
        "print(w2v_model.wv['nlp'])\n",
        "\n",
        "print(\"\\nVector size:\", w2v_model.wv.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK87RaxXAFXq",
        "outputId": "17bab628-4a77-419b-eec7-7610b42d5611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words similar to 'ai':\n",
            "[('fascinating', 0.21886461973190308), ('part', 0.21618622541427612), ('the', 0.09310683608055115), ('using', 0.09290151298046112), ('world', 0.07948420941829681), ('machine', 0.06284788995981216), ('field', 0.05455850437283516), ('we', 0.027049632743000984), ('python', 0.016147596761584282), ('nlp', -0.010376579128205776)]\n"
          ]
        }
      ],
      "source": [
        "#Similar Words using Word2Vec\n",
        "similar_words = w2v_model.wv.most_similar('ai')\n",
        "print(\"Words similar to 'ai':\")\n",
        "print(similar_words)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
