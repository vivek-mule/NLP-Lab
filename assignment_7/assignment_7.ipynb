{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 7\n",
        "\n",
        "Name: Vivek Mule\n",
        "Roll: 381072\n",
        "PRN: 22420145\n",
        "\n",
        "#### Develop a text preprocessing and analysis application using NLTK for tokenization, POS tagging, and basic NLP tasks.  "
      ],
      "metadata": {
        "id": "_3IQlyRXu5hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP-q9NSTvGXs",
        "outputId": "410eadb0-b6e6-4c84-b414-b2aef151b9e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "\n",
        "# Downloading required datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPjLnvb1vHrW",
        "outputId": "8584d7d1-8cf9-4454-e438-60b0a6091bd8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Natural language processing (NLP) is technology that allows computers to interpret, manipulate, and comprehend human language. Organizations today have large volumes of voice and text data from various communication channels like emails, text messages, social media newsfeeds, video, audio, and more.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Text:\\n\")\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scvI6KcYvI5U",
        "outputId": "253adec7-86fd-4c86-a952-f0e7fee5e5c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "\n",
            "Natural language processing (NLP) is technology that allows computers to interpret, manipulate, and comprehend human language. Organizations today have large volumes of voice and text data from various communication channels like emails, text messages, social media newsfeeds, video, audio, and more.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"Sentence Tokenization:\\n\")\n",
        "for s in sentences:\n",
        "    print(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SvtmBOEvKY5",
        "outputId": "e8971915-77f6-4e94-f19a-b2127663520d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "\n",
            "\n",
            "Natural language processing (NLP) is technology that allows computers to interpret, manipulate, and comprehend human language.\n",
            "Organizations today have large volumes of voice and text data from various communication channels like emails, text messages, social media newsfeeds, video, audio, and more.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"Word Tokenization:\\n\")\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msFprJJjvMlL",
        "outputId": "333bcdfe-08cd-48ca-edb9-db990a91d028"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization:\n",
            "\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'technology', 'that', 'allows', 'computers', 'to', 'interpret', ',', 'manipulate', ',', 'and', 'comprehend', 'human', 'language', '.', 'Organizations', 'today', 'have', 'large', 'volumes', 'of', 'voice', 'and', 'text', 'data', 'from', 'various', 'communication', 'channels', 'like', 'emails', ',', 'text', 'messages', ',', 'social', 'media', 'newsfeeds', ',', 'video', ',', 'audio', ',', 'and', 'more', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "filtered_words = [\n",
        "    word for word in words\n",
        "    if word.lower() not in stop_words and word not in string.punctuation\n",
        "]\n",
        "\n",
        "print(\"After Stopword Removal:\\n\")\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUDxpcfcvt_5",
        "outputId": "6c8a0b76-8b0f-4a71-b7ce-3a031ac3fced"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stopword Removal:\n",
            "\n",
            "['Natural', 'language', 'processing', 'NLP', 'technology', 'allows', 'computers', 'interpret', 'manipulate', 'comprehend', 'human', 'language', 'Organizations', 'today', 'large', 'volumes', 'voice', 'text', 'data', 'various', 'communication', 'channels', 'like', 'emails', 'text', 'messages', 'social', 'media', 'newsfeeds', 'video', 'audio']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "print(\"POS Tagging:\\n\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdVq-JTPvvqc",
        "outputId": "5c900faf-a960-49e6-a042-e7353efb0098"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging:\n",
            "\n",
            "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('technology', 'NN'), ('that', 'WDT'), ('allows', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('interpret', 'VB'), (',', ','), ('manipulate', 'VB'), (',', ','), ('and', 'CC'), ('comprehend', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), ('Organizations', 'NNS'), ('today', 'NN'), ('have', 'VBP'), ('large', 'JJ'), ('volumes', 'NNS'), ('of', 'IN'), ('voice', 'NN'), ('and', 'CC'), ('text', 'NN'), ('data', 'NNS'), ('from', 'IN'), ('various', 'JJ'), ('communication', 'NN'), ('channels', 'NNS'), ('like', 'IN'), ('emails', 'NNS'), (',', ','), ('text', 'NN'), ('messages', 'NNS'), (',', ','), ('social', 'JJ'), ('media', 'NNS'), ('newsfeeds', 'NNS'), (',', ','), ('video', 'NN'), (',', ','), ('audio', 'NN'), (',', ','), ('and', 'CC'), ('more', 'JJR'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist = FreqDist(filtered_words)\n",
        "\n",
        "print(\"Most Common Words:\\n\")\n",
        "print(fdist.most_common(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bsLDGlXvxTu",
        "outputId": "2a4698c6-4b0a-4d11-a1e7-e11f2df59c31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common Words:\n",
            "\n",
            "[('language', 2), ('text', 2), ('Natural', 1), ('processing', 1), ('NLP', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KyCLZuVkwHpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}