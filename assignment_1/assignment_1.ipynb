{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmrwWMEgowtt"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "Name: Vivek Mule\n",
        "Roll: 381072\n",
        "PRN: 22420145\n",
        "\n",
        "Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK\n",
        "library. Use porter stemmer and snowball stemmer for stemming. Use any technique for\n",
        "lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeOCT19bo45i",
        "outputId": "79b618e9-f1c5-410d-cf85-8261b9d1073c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz3CSD2_pT_V",
        "outputId": "0b01c031-3815-4fbc-c1ef-a3baf516acfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download NLTK Resources\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD8iTLw0pXw1",
        "outputId": "6f8d46e3-0005-401d-cddc-50b401d9f748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Tokenization using NLTK in Python.\n"
          ]
        }
      ],
      "source": [
        "# Sample Text\n",
        "text = \"Performing Tokenization using NLTK in Python.\"\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1xjojwepZZR",
        "outputId": "a6f07ed2-253e-4068-9382-01f60602cc5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whitespace Tokenization: ['Performing', 'Tokenization', 'using', 'NLTK', 'in', 'Python.']\n"
          ]
        }
      ],
      "source": [
        "# Whitespace Tokenization\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "tokens_whitespace = wt.tokenize(text)\n",
        "print(\"Whitespace Tokenization:\", tokens_whitespace)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p_todPtpZ5t",
        "outputId": "acd5a25c-487b-493e-c579-cc3129ef9448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Punctuation-based Tokenization: ['Performing', 'Tokenization', 'using', 'NLTK', 'in', 'Python', '.']\n"
          ]
        }
      ],
      "source": [
        "# Punctuation-based Tokenization\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "pt = WordPunctTokenizer()\n",
        "tokens_punct = pt.tokenize(text)\n",
        "print(\"Punctuation-based Tokenization:\", tokens_punct)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETToI4nKpgly",
        "outputId": "e8533514-6670-41f7-ee70-78174428eadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treebank Tokenization: ['Performing', 'Tokenization', 'using', 'NLTK', 'in', 'Python', '.']\n"
          ]
        }
      ],
      "source": [
        "# Treebank Tokenization\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tbt = TreebankWordTokenizer()\n",
        "tokens_treebank = tbt.tokenize(text)\n",
        "print(\"Treebank Tokenization:\", tokens_treebank)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUNEBcnBpirg",
        "outputId": "600b3a14-8d62-4994-a042-c4eabdc23eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tweet Tokenization: ['Performing', 'Tokenization', 'using', 'NLTK', 'in', 'Python', '.']\n"
          ]
        }
      ],
      "source": [
        "# Tweet Tokenization\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "tokens_tweet = tt.tokenize(text)\n",
        "print(\"Tweet Tokenization:\", tokens_tweet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1l_0n7jpjMU",
        "outputId": "202f33da-b785-4ff5-b3fd-30f653c06c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MWE Tokenization: ['performing', 'tokenization', 'using', 'nltk', 'in', 'python.']\n"
          ]
        }
      ],
      "source": [
        "# Multi-Word Expression (MWE) Tokenization\n",
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('let', \"'s\"), ('openai',)])\n",
        "tokens_mwe = mwe.tokenize(text.lower().split())\n",
        "print(\"MWE Tokenization:\", tokens_mwe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20wz9ZtgpkZl",
        "outputId": "434b3c8d-38fb-47a8-9f9a-2c6b3a514d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porter Stemmer Output: ['perform', 'token', 'use', 'nltk', 'in', 'python', '.']\n"
          ]
        }
      ],
      "source": [
        "# Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "porter_stems = [ps.stem(word) for word in tokens_treebank]\n",
        "print(\"Porter Stemmer Output:\", porter_stems)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHiQbbPbplvN",
        "outputId": "726a141f-35d9-4c05-9d00-b5f8cdb7b09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snowball Stemmer Output: ['perform', 'token', 'use', 'nltk', 'in', 'python', '.']\n"
          ]
        }
      ],
      "source": [
        "# Snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "snowball_stems = [ss.stem(word) for word in tokens_treebank]\n",
        "print(\"Snowball Stemmer Output:\", snowball_stems)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCcfX9_npm8t",
        "outputId": "970f4634-10ae-4e71-9f1b-3616bc4ad8b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatization Output: ['Performing', 'Tokenization', 'using', 'NLTK', 'in', 'Python', '.']\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization (WordNet Lemmatizer)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_treebank]\n",
        "print(\"Lemmatization Output:\", lemmatized_words)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
